{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>File No.</th>\n",
       "      <th>Opened</th>\n",
       "      <th>Closed</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>SubCoverage</th>\n",
       "      <th>Reason</th>\n",
       "      <th>SubReason</th>\n",
       "      <th>Disposition</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Recovery</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oxford Health Plans (CT), Inc</td>\n",
       "      <td>7054984</td>\n",
       "      <td>06/08/2023</td>\n",
       "      <td>06/13/2023</td>\n",
       "      <td>Group</td>\n",
       "      <td>Health Only</td>\n",
       "      <td>Claim Handling</td>\n",
       "      <td>Medical Necessity</td>\n",
       "      <td>Company Position Substantiated</td>\n",
       "      <td>Coverage Denied</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ConnectiCare Benefits, Inc.</td>\n",
       "      <td>7046842</td>\n",
       "      <td>07/27/2022</td>\n",
       "      <td>08/31/2022</td>\n",
       "      <td>A &amp; H</td>\n",
       "      <td>Exchange</td>\n",
       "      <td>Claim Handling</td>\n",
       "      <td>Unsatisfactory Settlement</td>\n",
       "      <td>Company Position Substantiated</td>\n",
       "      <td>Company Position Upheld</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ReliaStar Life Insurance Company</td>\n",
       "      <td>7056274</td>\n",
       "      <td>08/01/2023</td>\n",
       "      <td>09/06/2023</td>\n",
       "      <td>Individual Annuities</td>\n",
       "      <td>Fixed</td>\n",
       "      <td>Claim Handling</td>\n",
       "      <td>Prompt Pay</td>\n",
       "      <td>Company Position Substantiated</td>\n",
       "      <td>Furnished Information</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anthem Health Plans, Inc</td>\n",
       "      <td>7045021</td>\n",
       "      <td>05/09/2022</td>\n",
       "      <td>06/09/2022</td>\n",
       "      <td>Group</td>\n",
       "      <td>A &amp; H</td>\n",
       "      <td>Claim Handling</td>\n",
       "      <td>UR MEDICALLY NECESSARY DENIAL</td>\n",
       "      <td>Company Position Substantiated</td>\n",
       "      <td>External Review Info Sent</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Underwriters at Lloyds London</td>\n",
       "      <td>7019177</td>\n",
       "      <td>05/13/2019</td>\n",
       "      <td>06/14/2019</td>\n",
       "      <td>Commercial Multi-Peril</td>\n",
       "      <td>Commercial Fire</td>\n",
       "      <td>Claim Handling</td>\n",
       "      <td>Unsatisfactory Settlement/Offer</td>\n",
       "      <td>Company Position Substantiated</td>\n",
       "      <td>Refer To Appraisal</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Company  File No.      Opened      Closed  \\\n",
       "0     Oxford Health Plans (CT), Inc   7054984  06/08/2023  06/13/2023   \n",
       "1       ConnectiCare Benefits, Inc.   7046842  07/27/2022  08/31/2022   \n",
       "2  ReliaStar Life Insurance Company   7056274  08/01/2023  09/06/2023   \n",
       "3          Anthem Health Plans, Inc   7045021  05/09/2022  06/09/2022   \n",
       "4     Underwriters at Lloyds London   7019177  05/13/2019  06/14/2019   \n",
       "\n",
       "                 Coverage      SubCoverage          Reason  \\\n",
       "0                   Group      Health Only  Claim Handling   \n",
       "1                   A & H         Exchange  Claim Handling   \n",
       "2    Individual Annuities            Fixed  Claim Handling   \n",
       "3                   Group            A & H  Claim Handling   \n",
       "4  Commercial Multi-Peril  Commercial Fire  Claim Handling   \n",
       "\n",
       "                         SubReason                     Disposition  \\\n",
       "0                Medical Necessity  Company Position Substantiated   \n",
       "1        Unsatisfactory Settlement  Company Position Substantiated   \n",
       "2                       Prompt Pay  Company Position Substantiated   \n",
       "3    UR MEDICALLY NECESSARY DENIAL  Company Position Substantiated   \n",
       "4  Unsatisfactory Settlement/Offer  Company Position Substantiated   \n",
       "\n",
       "                  Conclusion  Recovery  Status  \n",
       "0            Coverage Denied       0.0  Closed  \n",
       "1    Company Position Upheld       0.0  Closed  \n",
       "2      Furnished Information       0.0  Closed  \n",
       "3  External Review Info Sent       0.0  Closed  \n",
       "4         Refer To Appraisal       0.0  Closed  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./subset_df_2.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 180 entries, 0 to 179\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   Company      180 non-null    category      \n",
      " 1   File No.     180 non-null    int64         \n",
      " 2   Opened       180 non-null    datetime64[ns]\n",
      " 3   Closed       180 non-null    datetime64[ns]\n",
      " 4   Coverage     180 non-null    category      \n",
      " 5   SubCoverage  180 non-null    category      \n",
      " 6   Reason       180 non-null    category      \n",
      " 7   SubReason    180 non-null    category      \n",
      " 8   Disposition  180 non-null    category      \n",
      " 9   Conclusion   180 non-null    category      \n",
      " 10  Recovery     180 non-null    float64       \n",
      " 11  Status       180 non-null    category      \n",
      "dtypes: category(8), datetime64[ns](2), float64(1), int64(1)\n",
      "memory usage: 12.3 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Convert 'Company', 'Coverage', 'SubCoverage', 'Reason', 'SubReason', 'Disposition', 'Conclusion', 'Status' to categorical\n",
    "for column in ['Company', 'Coverage', 'SubCoverage', 'Reason', 'SubReason', 'Disposition', 'Conclusion', 'Status']:\n",
    "  data[column] = pd.Categorical(data[column])\n",
    "\n",
    "# Convert 'Opened' and 'Closed' to datetime\n",
    "data['Opened'] = pd.to_datetime(data['Opened'], errors='coerce')\n",
    "data['Closed'] = pd.to_datetime(data['Closed'], errors='coerce')\n",
    "\n",
    "# Convert 'Recovery' to float\n",
    "data['Recovery'] = pd.to_numeric(data['Recovery'], errors='coerce')\n",
    "\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curate Data in Query and Response structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Question and response can be added to make chat result better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_data = data.copy()\n",
    "\n",
    "# Generate query-response pairs\n",
    "query_response_data = []\n",
    "\n",
    "for idx, row in claim_data.iterrows():\n",
    "    # Query 1: Claim status\n",
    "    query_response_data.append({\n",
    "        'query': f\"What is the status of my claim with file number {row['File No.']}?\",\n",
    "        'response': f\"Your claim with File No. {row['File No.']} is currently {row['Status']}.\"\n",
    "    })\n",
    "\n",
    "    # # Query 2: Recovery amount\n",
    "    # query_response_data.append({\n",
    "    #     'query': f\"What was the recovery amount for claim number {row['File No.']}?\",\n",
    "    #     'response': f\"The recovery amount for your claim with File No. {row['File No.']} is ${row['Recovery']}.\"\n",
    "    # })\n",
    "\n",
    "    # Query 3: Denial reason (if applicable)\n",
    "    if row['Disposition'] == 'Claim Settled':\n",
    "        query_response_data.append({\n",
    "            'query': f\"Why was my claim with file number {row['File No.']} Claim Settled?\",\n",
    "            'response': f\"Your claim with File No. {row['File No.']} was Settled on  {row['Closed']}.\"\n",
    "        })\n",
    "\n",
    "    # Query 4: Claim open and close dates\n",
    "    query_response_data.append({\n",
    "        'query': f\"When was my claim with file number {row['File No.']} opened and closed?\",\n",
    "        'response': f\"Your claim with File No. {row['File No.']} was opened on {row['Opened']} and closed on {row['Closed']}.\"\n",
    "    })\n",
    "\n",
    "    # # Query 5: Disposition reason\n",
    "    # query_response_data.append({\n",
    "    #     'query': f\"Why is my claim with file number {row['File No.']} {row['Disposition']}?\",\n",
    "    #     'response': f\"Your claim with File No. {row['File No.']} was {row['Disposition']} due to {row['SubReason']}.\"\n",
    "    # })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving query_response file for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the status of my claim with file numbe...</td>\n",
       "      <td>Your claim with File No. 7054984 is currently ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When was my claim with file number 7054984 ope...</td>\n",
       "      <td>Your claim with File No. 7054984 was opened on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the status of my claim with file numbe...</td>\n",
       "      <td>Your claim with File No. 7046842 is currently ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When was my claim with file number 7046842 ope...</td>\n",
       "      <td>Your claim with File No. 7046842 was opened on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the status of my claim with file numbe...</td>\n",
       "      <td>Your claim with File No. 7056274 is currently ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  What is the status of my claim with file numbe...   \n",
       "1  When was my claim with file number 7054984 ope...   \n",
       "2  What is the status of my claim with file numbe...   \n",
       "3  When was my claim with file number 7046842 ope...   \n",
       "4  What is the status of my claim with file numbe...   \n",
       "\n",
       "                                            response  \n",
       "0  Your claim with File No. 7054984 is currently ...  \n",
       "1  Your claim with File No. 7054984 was opened on...  \n",
       "2  Your claim with File No. 7046842 is currently ...  \n",
       "3  Your claim with File No. 7046842 was opened on...  \n",
       "4  Your claim with File No. 7056274 is currently ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained vectorizer as a pickle file\n",
    "with open('query_response_data.pkl', 'wb') as f:\n",
    "    pickle.dump(query_response_data, f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "query_response_df = pd.DataFrame(query_response_data)\n",
    "query_response_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = query_response_data # You can use pickle file as well\n",
    "\n",
    "\n",
    "# Load GPT-Neo model and tokenizer\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"  # You can choose a larger model like gpt-neo-1.3B if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = query_response_df\n",
    "# Create a prompt with a few examples for in-context learning\n",
    "def generate_prompt(user_input):\n",
    "    examples = \"\"\n",
    "    for i in range(min(len(df), 3)):  # Include 3 examples from the dataframe\n",
    "        examples += f\"Q: {df['query'].iloc[i]}\\nA: {df['response'].iloc[i]}\\n\\n\"\n",
    "\n",
    "    # Combine examples with the user's question\n",
    "    prompt = examples + f\"Q: {user_input}\\nA:\"\n",
    "    return prompt\n",
    "\n",
    "# Function to generate response using GPT-Neo\n",
    "def gpt_neo_response(user_input):\n",
    "    prompt = generate_prompt(user_input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate response\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode the generated response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # Extract only the answer from the response (after the user's query)\n",
    "    response = response.split(\"A:\")[-1].strip()\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference for GPT Neo - One Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your claim with File No. 7066579 is currently Closed.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"What is status of file 7066579\"\n",
    "response = gpt_neo_response(user_input)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Custom Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofQuestion = query_response_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert list of Query Response to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to the desired text format\n",
    "with open(\"query_response_dataset.txt\", \"w\") as file:\n",
    "    for item in listofQuestion:\n",
    "        query = item['query']\n",
    "        response = item['response']\n",
    "        file.write(f\"Q: {query}\\nA: {response}\\n<|endoftext|>\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(filename, num_sentences=3):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Split the text into sentences (assuming sentences end with a period)\n",
    "    sentences = text.split('. ')\n",
    "    \n",
    "    # Return the first 'num_sentences' sentences\n",
    "    return '. '.join(sentences[:num_sentences]) + '.'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the status of my claim with file number 7054984?\n",
      "A: Your claim with File No. 7054984 is currently Closed.\n",
      "<|endoftext|>\n",
      "Q: When was my claim with file number 7054984 opened and closed?\n",
      "A: Your claim with File No. 7054984 was opened on 2023-06-08 00:00:00 and closed on 2023-06-13 00:00:00.\n",
      "<|endoftext|>\n",
      "Q: What is the status of my claim with file number 7046842?\n",
      "A: Your claim with File No. 7046842 is currently Closed.\n",
      "<|endoftext|>\n",
      "Q: When was my claim with file number 7046842 opened and closed?\n",
      "A: Your claim with File No. 7046842 was opened on 2022-07-27 00:00:00 and closed on 2022-08-31 00:00:00.\n",
      "<|endoftext|>\n",
      "Q: What is the status of my claim with file number 7056274?\n",
      "A: Your claim with File No.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "filename = 'query_response_dataset.txt'\n",
    "sentences = read_sentences(filename, num_sentences=5)  # Read first 5 sentences\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPTNeoForCausalLM, AutoTokenizer, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"  # Change to a larger model if needed (e.g., gpt-neo-1.3B)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Function to load and tokenize dataset\n",
    "def load_dataset(file_path, tokenizer, block_size=512):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "# Prepare data collator\n",
    "def prepare_collator(tokenizer):\n",
    "    return DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=False  # GPT-Neo does causal (autoregressive) language modeling, so MLM is False\n",
    "    )\n",
    "\n",
    "# Load the dataset (assuming your data is in a text file)\n",
    "train_dataset = load_dataset(\"./query_response_dataset.txt\", tokenizer)\n",
    "\n",
    "# Prepare the data collator\n",
    "data_collator = prepare_collator(tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt_neo_finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,          # You can adjust the number of epochs\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,              # Save checkpoint every 500 steps\n",
    "    save_total_limit=2,          # Only keep 2 last checkpoints\n",
    "    logging_dir=\"./logs\",        # Directory for logging\n",
    "    logging_steps=10,            # Log every 10 steps\n",
    "    learning_rate=5e-5,          # Fine-tuning learning rate\n",
    "    weight_decay=0.01,           # Weight decay for regularization\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start the fine-tuning process\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "trainer.save_model(\"./gpt_neo_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./gpt_neo_finetuned_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./gpt_neo_finetuned_model\"  # Path to your fine-tuned model\n",
    "tokenizer_path = \"./gpt_neo_finetuned_tokenizer\"  # Path to your fine-tuned tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the padding token to eos_token (or add a new pad_token if desired)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
    "    # Alternatively, you can define a new padding token\n",
    "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Ensure the model knows about the new special token if one was added\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Function to generate a prompt for the model\n",
    "def generate_prompt(user_input):\n",
    "    # Customize the prompt to follow your query-response format\n",
    "    prompt = f\"Q: {user_input}\\nA:\"\n",
    "    return prompt\n",
    "\n",
    "# Function to check if response is valid or needs fallback\n",
    "def is_valid_response(response):\n",
    "    # Check if the response is long enough and doesn't just repeat the question\n",
    "    min_length = 10  # Minimum length for a valid response\n",
    "    if len(response) < min_length:\n",
    "        return False\n",
    "    \n",
    "    # You can add other heuristics, e.g., if response contains certain phrases, return False\n",
    "    if \"I don't know\" in response or response.strip() == \"\":\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# Function to generate response using the fine-tuned GPT-Neo model\n",
    "def gpt_neo_response(user_input, max_new_tokens=50, temperature=0.7, top_p=0.9, top_k=50):\n",
    "    # Prepare the prompt\n",
    "    prompt = generate_prompt(user_input)\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    \n",
    "    # Generate response\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,  # Control how many tokens are generated\n",
    "        do_sample=True,                 # Use sampling for diverse responses\n",
    "        temperature=temperature,        # Control randomness of the output\n",
    "        top_p=top_p,                    # Nucleus sampling to focus on the top probability mass\n",
    "        top_k=top_k,                    # Limits sampling to top-k tokens\n",
    "        pad_token_id=tokenizer.pad_token_id,  # Use the defined padding token\n",
    "        #attention_mask=inputs['attention_mask']\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens into text\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the relevant answer (anything after \"A:\")\n",
    "    if \"A:\" in response:\n",
    "        response = response.split(\"A:\")[-1].strip()\n",
    "    \n",
    "    # Check if the response is valid, otherwise return a fallback message\n",
    "    if not is_valid_response(response):\n",
    "        response = \"I'm sorry, I didn't understand that. Could you please rephrase your question?\"\n",
    "\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your claim with File No. 7057039 is currently Reopened.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_input = \"What is the status of my claim with file number 7057039?\"\n",
    "response = gpt_neo_response(user_input)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_rag_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
